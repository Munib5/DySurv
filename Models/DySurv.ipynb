{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a95eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "\n",
    "import torch # For building the networks \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchtuples as tt # Some useful functions\n",
    "\n",
    "from pycox.models import LogisticHazard\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "import seaborn as sn\n",
    "sn.set_theme(style=\"white\", palette=\"rocket_r\")\n",
    "\n",
    "import random\n",
    "\n",
    "seed = 100 #(1024, 85858, 3673, 32)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "_ = torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5d9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the time-series\n",
    "def timeseries_processing(timeseries, labels, hour):\n",
    "    timeseries.reset_index(level=1, inplace=True)\n",
    "    timeseries.time = pd.to_timedelta(timeseries.time, errors='coerce')\n",
    "    \n",
    "    # Convert the time-stamps into hours\n",
    "    timeseries.time = timeseries.time.astype(int)/(1000000000*24)\n",
    "        \n",
    "    # Add time of event\n",
    "    timeseries = timeseries.merge(labels, left_index=True, right_index=True)\n",
    "    \n",
    "    # Only keep those timestamps with more than time before the event for prediction\n",
    "    timeseries = timeseries[timeseries['time'] <= (timeseries['actualiculos'] - hour)]\n",
    "    \n",
    "    # Drop the labels column, we will add them at the end for consistency and to avoid redundancy\n",
    "    timeseries = timeseries.drop(columns=['actualiculos', 'actualhospitalmortality', 'uniquepid', 'patienthealthsystemstayid'])\n",
    "    \n",
    "    timeseries.reset_index(inplace=True)\n",
    "    timeseries.set_index(['patient', 'time'], inplace=True)\n",
    "    \n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1731152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the static features\n",
    "labels = pd.read_csv('preprocessed_labels.csv', index_col='patient')\n",
    "labels['actualiculos'] = labels['actualiculos']*24\n",
    "labels.drop(labels.loc[labels['actualiculos']>240].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f26a901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the time-series data\n",
    "timeseries = pd.read_csv('preprocessed_timeseries.csv', index_col=['patient', 'time'])\n",
    "# Only take values up to 24 hours before event\n",
    "timeseries_summary = timeseries_processing(timeseries, labels, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495184da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/zrr_wqjs59v2n19lg6kssgnw0000gr/T/ipykernel_55498/204231096.py:3: FutureWarning: Using the level keyword in DataFrame and Series aggregations is deprecated and will be removed in a future version. Use groupby instead. df.median(level=1) should use df.groupby(level=1).median().\n",
      "  minimum_shifts = timeseries_summary.time.min(level=0)\n"
     ]
    }
   ],
   "source": [
    "# Time shift so that the starting point for each sample is time = 0\n",
    "timeseries_summary.reset_index(level=1, inplace=True)\n",
    "minimum_shifts = timeseries_summary.time.min(level=0)\n",
    "timeseries_summary = timeseries_summary.merge(minimum_shifts, left_index=True, right_index=True)\n",
    "timeseries_summary['time'] = timeseries_summary['time_x'] - timeseries_summary['time_y']\n",
    "timeseries_summary.drop(columns=['time_x', 'time_y'], inplace=True)\n",
    "timeseries_summary.set_index(['time'], append=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5257c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward (for measurements up until time-to-event) and backward (when the first couple of measurements are missing) fill\n",
    "timeseries_summary.update(timeseries_summary.groupby(level=0).ffill())\n",
    "timeseries_summary.update(timeseries_summary.groupby(level=0).bfill())\n",
    "\n",
    "timeseries_summary.ffill(axis = 0, inplace=True)\n",
    "timeseries_summary.bfill(axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d8f63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import static features\n",
    "statics = pd.read_csv('preprocessed_flat.csv', index_col='patient')\n",
    "statics.drop('nullheight', axis=1, inplace=True)\n",
    "timeseries_summary = timeseries_summary.merge(statics, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "323b5c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_labels = labels.loc[list(set(list(timeseries_summary.index.get_level_values('patient'))))]\n",
    "\n",
    "# Create validation and test set (20% each)\n",
    "df_test = sample_labels.sample(frac=0.2)\n",
    "df_train = sample_labels.drop(df_test.index)\n",
    "df_val = sample_labels.sample(frac=0.2)\n",
    "df_train = sample_labels.drop(df_val.index)\n",
    "\n",
    "st = set(list(df_val.index.get_level_values('patient')))\n",
    "val_index = [i for i, e in enumerate(list(set(list(timeseries_summary.index.get_level_values('patient'))))) if e in st]\n",
    "st = set(list(df_train.index.get_level_values('patient')))\n",
    "train_index = [i for i, e in enumerate(list(set(list(timeseries_summary.index.get_level_values('patient'))))) if e in st]\n",
    "st = set(list(df_test.index.get_level_values('patient')))\n",
    "test_index = [i for i, e in enumerate(list(set(list(timeseries_summary.index.get_level_values('patient'))))) if e in st]\n",
    "\n",
    "# Extract time-to-event and event label\n",
    "num_durations = 10\n",
    "labtrans = LogisticHazard.label_transform(num_durations)\n",
    "get_target = lambda df: (df['actualiculos'].values, df['actualhospitalmortality'].values)\n",
    "y_train_surv = labtrans.fit_transform(*get_target(df_train))\n",
    "y_val_surv = labtrans.transform(*get_target(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72cb1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time-series input for LSTM of shape [n, timestep, features]\n",
    "def split_sequence(dataframe, n_steps):\n",
    "    lstm_input = np.empty((len(dataframe.index.levels[0]), n_steps, 98)) #98, 65\n",
    "    lstm_input[:] = np.nan\n",
    "    for i in range(len(dataframe.index.levels[0])):\n",
    "        sample = dataframe.loc[dataframe.index.levels[0][i].tolist()]\n",
    "        sequence = sample.to_numpy()\n",
    "        n_features = sequence.shape[1]\n",
    "        time_length = sequence.shape[0]\n",
    "\n",
    "        if n_steps > time_length:\n",
    "            a = np.empty((n_steps-time_length,n_features))\n",
    "            for j in range((n_steps-time_length)):\n",
    "                a[j, :] = sequence[0, :]\n",
    "            sequence = np.vstack((a,sequence))\n",
    "        sequence = sequence[-n_steps:, :]\n",
    "        lstm_input[i, :, :] = sequence\n",
    "    \n",
    "    return lstm_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725504a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data into LSTM timeseries format with 72 1-hour timesteps\n",
    "timeseries_lstm_input = split_sequence(timeseries_summary, 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d728de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_lstm_input_train = timeseries_lstm_input[train_index, :, :]\n",
    "timeseries_lstm_input_val = timeseries_lstm_input[val_index, :, :]\n",
    "timeseries_lstm_input_test = timeseries_lstm_input[test_index, :, :]\n",
    "\n",
    "# Quantile transform the features across the population for each timestep\n",
    "scalers = {}\n",
    "for i in range(timeseries_lstm_input_train.shape[1]):\n",
    "    scalers[i] = QuantileTransformer(random_state=0)\n",
    "    timeseries_lstm_input_train[:, i, :] = scalers[i].fit_transform(timeseries_lstm_input_train[:, i, :]) \n",
    "    \n",
    "for i in range(timeseries_lstm_input_val.shape[1]):\n",
    "    timeseries_lstm_input_val[:, i, :] = scalers[i].transform(timeseries_lstm_input_val[:, i, :]) \n",
    "\n",
    "for i in range(timeseries_lstm_input_test.shape[1]):\n",
    "    timeseries_lstm_input_test[:, i, :] = scalers[i].transform(timeseries_lstm_input_test[:, i, :]) \n",
    "\n",
    "x_train = timeseries_lstm_input_train.astype('float32')\n",
    "x_val = timeseries_lstm_input_val.astype('float32')\n",
    "x_test = timeseries_lstm_input_test.astype('float32')\n",
    "\n",
    "train_target = np.zeros((x_train.shape[0], x_train.shape[1], 1))\n",
    "train_target[:, :, -1] = y_train_surv[0].reshape(-1, 1)\n",
    "x_train = np.append(x_train, train_target, axis=2)\n",
    "\n",
    "val_target = np.zeros((x_val.shape[0], x_val.shape[1], 1))\n",
    "val_target[:, :, -1] = y_val_surv[0].reshape(-1, 1)\n",
    "x_val = np.append(x_val, val_target, axis=2)\n",
    "\n",
    "train = tt.tuplefy(x_train, (y_train_surv, x_train))\n",
    "val = tt.tuplefy(x_val, (y_val_surv, x_val))\n",
    "\n",
    "# We don't need to transform the test labels\n",
    "durations_test, events_test = get_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "387da0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        if tensor.dim() == 2:\n",
    "            return tensor[:, :]\n",
    "        return tensor[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c404c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, no_features, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.no_features = no_features\n",
    "        self.hidden_size = (2 * no_features)\n",
    "        self.output_size = output_size\n",
    "        self.LSTM1 = nn.LSTM(\n",
    "            input_size = no_features,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_size, 3*self.hidden_size)\n",
    "        self.fc2 = nn.Linear(3*self.hidden_size, 5*self.hidden_size)\n",
    "        self.fc3 = nn.Linear(5*self.hidden_size, 3*self.hidden_size)\n",
    "        self.fc4 = nn.Linear(3*self.hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat((x, y.reshape(-1, 1)), dim=1)\n",
    "        x = x.unsqueeze(1).repeat(1, self.seq_len, 1)\n",
    "        x, (hidden_state, cell_state) = self.LSTM1(x)\n",
    "        x = x.reshape((-1, self.seq_len, self.hidden_size))\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        x = self.dropout(self.fc3(x))\n",
    "        out = self.fc4(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dd6c0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySurv(nn.Module):\n",
    "    def __init__(self, in_features, encoded_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(in_features, in_features, batch_first=True)\n",
    "        self.extract = extract_tensor()\n",
    "        self.fc11 = nn.Linear(in_features, 3*in_features)\n",
    "        self.fc12 = nn.Linear(3*in_features, 5*in_features)\n",
    "        self.fc13 = nn.Linear(5*in_features, 3*in_features)\n",
    "        self.fc14 = nn.Linear(3*in_features, encoded_features)\n",
    "\n",
    "        self.fc24 = nn.Linear(3*in_features, encoded_features)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "        self.surv_net = nn.Sequential(\n",
    "            nn.Linear(encoded_features, 3*in_features), nn.ReLU(),\n",
    "            nn.Linear(3*in_features, 5*in_features), nn.ReLU(),\n",
    "            nn.Linear(5*in_features, 3*in_features), nn.ReLU(),\n",
    "            nn.Linear(3*in_features, out_features),\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = Decoder(72, encoded_features+1, in_features)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = std.data.new(std.size()).normal_()\n",
    "        sample_z = eps.mul(std).add_(mu)\n",
    "\n",
    "        return sample_z\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        x = self.lstm1(x)\n",
    "        x = self.relu(self.fc11(self.extract(x)))\n",
    "        x = self.relu(self.fc12(x))\n",
    "        x = self.relu(self.fc13(x))\n",
    "        mu_z = self.fc14(x)\n",
    "        logvar_z = self.fc24(x)\n",
    "\n",
    "        return mu_z, logvar_z\n",
    "    \n",
    "    def forward(self, input):\n",
    "        y = input[:, -1, 98]\n",
    "        x = input[:, :, :98]\n",
    "        mu, logvar = self.encoder(x.float())\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.decoder2(z, y.float()), self.surv_net(z), mu, logvar\n",
    "\n",
    "    def predict(self, input):\n",
    "        mu, logvar = self.encoder(input)\n",
    "        encoded = self.reparameterize(mu, logvar)\n",
    "        return self.surv_net(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb446729",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = x_train.shape[2]-1\n",
    "encoded_features = 20 # use 20 latent factors\n",
    "out_features = labtrans.out_features # how many discrete time points to predict for (10 here)\n",
    "net = DySurv(in_features, encoded_features, out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6927e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(torch.nn.Module):\n",
    "    def __init__(self, reduction: str = 'mean') -> None:\n",
    "        super().__init__()\n",
    "        self.reduction = reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_logistic_hazard(phi: Tensor, idx_durations: Tensor, events: Tensor,\n",
    "                        reduction: str = 'mean') -> Tensor:\n",
    "    \"\"\"\n",
    "    References:\n",
    "    [1] Håvard Kvamme and Ørnulf Borgan. Continuous and Discrete-Time Survival Prediction\n",
    "        with Neural Networks. arXiv preprint arXiv:1910.06724, 2019.\n",
    "        https://arxiv.org/pdf/1910.06724.pdf\n",
    "    \"\"\"\n",
    "    if phi.shape[1] <= idx_durations.max():\n",
    "        raise ValueError(f\"Network output `phi` is too small for `idx_durations`.\"+\n",
    "                         f\" Need at least `phi.shape[1] = {idx_durations.max().item()+1}`,\"+\n",
    "                         f\" but got `phi.shape[1] = {phi.shape[1]}`\")\n",
    "    if events.dtype is torch.bool:\n",
    "        events = events.float()\n",
    "    events = events.view(-1, 1)\n",
    "    idx_durations = idx_durations.view(-1, 1)\n",
    "    y_bce = torch.zeros_like(phi).scatter(1, idx_durations, events)\n",
    "    bce = F.binary_cross_entropy_with_logits(phi, y_bce, reduction='none')\n",
    "    loss = bce.cumsum(1).gather(1, idx_durations).view(-1)\n",
    "    return _reduction(loss, reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLLLogistiHazardLoss(_Loss):\n",
    "    def forward(self, phi: Tensor, idx_durations: Tensor, events: Tensor) -> Tensor:\n",
    "        return nll_logistic_hazard(phi, idx_durations, events, self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75c584e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, alpha: list):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.loss_surv = NLLLogistiHazardLoss()\n",
    "        self.loss_ae = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, decoded, phi, mu, logvar, target_loghaz, target_ae):\n",
    "        idx_durations, events = target_loghaz\n",
    "        target_ae = target_ae[:, :, :98].float()\n",
    "        loss_surv = self.loss_surv(phi, idx_durations, events)/10\n",
    "        loss_ae = self.loss_ae(decoded, target_ae)/1\n",
    "        loss_kd = (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()))/10\n",
    "        return self.alpha[0] * loss_surv + self.alpha[1] * loss_ae + self.alpha[2] * loss_kd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a44b673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = Loss([0.333, 0.333, 0.333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d81df5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticHazard(net, tt.optim.Adam(0.0001), duration_index=labtrans.cuts, loss=loss) # wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7228d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = dict(\n",
    "    loss_surv = LossAELogHaz([1, 0, 0]),\n",
    "    loss_ae = LossAELogHaz([0, 1, 0]),\n",
    "    loss_kd = LossAELogHaz([0, 0, 1])\n",
    ")\n",
    "callbacks = [tt.cb.EarlyStopping()]\n",
    "# callbacks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 100\n",
    "log = model.fit(*train, batch_size = batch_size, epochs = epochs, callbacks = callbacks, verbose = False, val_data=val, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90260bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.log.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6335493",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = res[['train_loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = res[['train_loss_surv', 'val_loss_surv']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = res[['train_loss_ae', 'val_loss_ae']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e58c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = res[['train_loss_kd', 'val_loss_kd']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff4e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = model.interpolate(10).predict_surv_df(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv.iloc[:, :5].plot(drawstyle='steps-post')\n",
    "plt.ylabel('S(t | x)')\n",
    "_ = plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d362fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9338cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.concordance_td('adj_antolini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28891f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
    "ev.brier_score(time_grid).plot()\n",
    "plt.ylabel('Brier score')\n",
    "_ = plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.nbll(time_grid).plot()\n",
    "plt.ylabel('NBLL')\n",
    "_ = plt.xlabel('Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_brier_score(time_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39269142",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.integrated_nbll(time_grid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a4254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
